import os
import time
import random
# ä¼˜å…ˆå°è¯•å¯¼å…¥æ–°çš„ HuggingFace åº“ï¼Œå¦‚æžœä¸å­˜åœ¨åˆ™ä½¿ç”¨ community
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from config import CHROMA_DIR, EMBEDDING_MODEL, OLLAMA_BASE_URL
from document_processor import DocumentProcessor

class KnowledgeBase:
    def __init__(self):
        # === ä¿®å¤æ ¸å¿ƒ 1ï¼šé€šè¿‡çŽ¯å¢ƒå˜é‡é…ç½® Ollama ===
        # LangChain çš„ OllamaEmbeddings ä¼šè‡ªåŠ¨è¯»å–è¿™äº›çŽ¯å¢ƒå˜é‡
        # è¿™æ ·å¯ä»¥ç»•è¿‡æž„é€ å‡½æ•°çš„å‚æ•°éªŒè¯é”™è¯¯
        os.environ['OLLAMA_BASE_URL'] = OLLAMA_BASE_URL
        os.environ['OLLAMA_HOST'] = OLLAMA_BASE_URL

        try:
            print(f"ðŸ”Œ æ­£åœ¨è¿žæŽ¥ Ollama ({OLLAMA_BASE_URL})...")
            # === ä¿®å¤æ ¸å¿ƒ 2ï¼šåˆå§‹åŒ–æ—¶ä¸ä¼  extra args ===
            self.embeddings = OllamaEmbeddings(
                model=EMBEDDING_MODEL
            )
            # ç®€å•çš„å†’çƒŸæµ‹è¯•ï¼Œç¡®ä¿æœåŠ¡çœŸçš„é€šäº†
            self.embeddings.embed_query("test")
            print("âœ… Ollama Embedding æœåŠ¡è¿žæŽ¥æˆåŠŸ")

        except Exception as e:
            print(f"âŒ Ollama è¿žæŽ¥å¤±è´¥: {e}")
            print("ðŸ’¡ è¯·æ£€æŸ¥ï¼š1. Ollama æ˜¯å¦å·²å¯åŠ¨ (ollama serve)ï¼Ÿ 2. æ¨¡åž‹æ˜¯å¦å·²ä¸‹è½½ (ollama pull nomic-embed-text)ï¼Ÿ")

            # === ä¿®å¤æ ¸å¿ƒ 3ï¼šä¸å†è‡ªåŠ¨ fallback åˆ° HuggingFace ===
            # å› ä¸ºå›½å†…ç½‘ç»œé€šå¸¸è¿žä¸ä¸Š HFï¼Œè‡ªåŠ¨ä¸‹è½½ä¼šå¯¼è‡´é•¿æ—¶é—´å¡æ­»
            # ç›´æŽ¥æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç”¨æˆ·å…ˆåŽ»ä¿®å¥½ Ollama
            raise RuntimeError("æ— æ³•è¿žæŽ¥åˆ°æœ¬åœ° Ollama Embedding æœåŠ¡ï¼Œä¸”æ— æ³•è¿žæŽ¥ HuggingFaceã€‚è¯·ä¼˜å…ˆç¡®ä¿ Ollama æ­£å¸¸è¿è¡Œã€‚")

        self.processor = DocumentProcessor()
        # åˆå§‹åŒ–å‘é‡åº“
        self.vectorstore = Chroma(
            persist_directory=CHROMA_DIR,
            embedding_function=self.embeddings,
            collection_name="feynman_knowledge_v3"
        )

    def add_document(self, file_path: str, subject: str = "é»˜è®¤") -> int:
        documents = self.processor.process_document(file_path, subject)
        if not documents:
            return 0

        # åˆ†æ‰¹æ¬¡æ’å…¥ï¼Œé˜²æ­¢çˆ†æ˜¾å­˜
        BATCH_SIZE = 10
        total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE

        print(f"æ­£åœ¨å¯¼å…¥ {len(documents)} ä¸ªçŸ¥è¯†å—ï¼Œåˆ† {total_batches} æ‰¹å¤„ç†...")

        success_count = 0
        for i in range(0, len(documents), BATCH_SIZE):
            batch = documents[i : i + BATCH_SIZE]
            try:
                self.vectorstore.add_documents(batch)
                success_count += len(batch)
                print(f"è¿›åº¦: {min(i + BATCH_SIZE, len(documents))}/{len(documents)}")
                # ç¨å¾®æš‚åœï¼Œç»™æ˜¾å¡å–˜æ¯æ—¶é—´
                time.sleep(0.1)
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {i//BATCH_SIZE + 1} å¯¼å…¥å¤±è´¥: {e}")

        return success_count

    def get_all_subjects(self) -> list:
        try:
            data = self.vectorstore.get()
            subjects = set()
            for metadata in data.get('metadatas', []):
                if metadata and 'subject' in metadata:
                    subjects.add(metadata['subject'])
            return sorted(list(subjects))
        except:
            return []

    def get_course_structure(self, subject: str) -> dict:
        collection = self.vectorstore._collection
        try:
            results = collection.get(
                where={"subject": subject},
                include=['metadatas', 'documents']
            )
        except Exception as e:
            print(f"æ•°æ®åº“è¯»å–é”™è¯¯: {e}")
            return {"subject": subject, "total_chunks": 0, "chapters": []}

        if not results['ids']:
            return {"subject": subject, "total_chunks": 0, "chapters": []}

        # æ¸…æ´—æ•°æ®
        valid_items = []
        ids = results['ids']
        metas = results['metadatas']
        docs = results['documents']

        for i in range(len(ids)):
            if not metas[i] or not docs[i]:
                continue
            valid_items.append((ids[i], metas[i], docs[i]))

        # æŽ’åº
        valid_items.sort(key=lambda x: x[1].get('chunk_id', 0))

        chapters = {}
        for pid, meta, doc in valid_items:
            source = meta.get('source', 'æœªçŸ¥ç« èŠ‚')
            if source not in chapters:
                chapters[source] = {
                    "chapter_id": len(chapters),
                    "title": source.replace('.pdf', '').replace('.docx', '').replace('.md', ''),
                    "source": source,
                    "chunks": []
                }

            chapters[source]["chunks"].append({
                "id": pid,
                "chunk_id": meta.get('chunk_id', 0),
                "preview": doc[:80].replace('\n', ' ') + "...",
                "content": doc,
                "metadata": meta
            })

        return {
            "subject": subject,
            "total_chunks": len(valid_items),
            "chapters": list(chapters.values())
        }

    def get_chapter_progress(self, subject: str, tracker) -> list:
        structure = self.get_course_structure(subject)
        progress_data = tracker.get_subject_progress(subject)
        progress_map = {p['knowledge_id']: p for p in progress_data}

        for chapter in structure['chapters']:
            completed = 0
            mastered = 0
            for chunk in chapter['chunks']:
                kid = chunk['id']
                if kid in progress_map:
                    chunk['progress'] = progress_map[kid]
                    completed += 1
                    if progress_map[kid]['mastery_level'] >= 0.8:
                        mastered += 1
                else:
                    chunk['progress'] = None

            chapter['stats'] = {
                'total': len(chapter['chunks']),
                'completed': completed,
                'mastered': mastered,
                'progress_pct': round(completed / len(chapter['chunks']) * 100) if chapter['chunks'] else 0
            }
        return structure

    def get_knowledge_by_id(self, k_id: str) -> dict:
        collection = self.vectorstore._collection
        results = collection.get(ids=[k_id], include=['metadatas', 'documents'])
        if results['ids']:
            return {
                "content": results['documents'][0],
                "metadata": results['metadatas'][0],
                "id": results['ids'][0]
            }
        return None

    def get_next_unlearned(self, subject: str, tracker) -> dict:
        structure = self.get_course_structure(subject)
        learned_ids = set(tracker.get_learned_ids(subject))

        for chapter in structure['chapters']:
            for chunk in chapter['chunks']:
                if chunk['id'] not in learned_ids:
                    return {
                        "content": chunk['content'],
                        "metadata": chunk['metadata'],
                        "id": chunk['id'],
                        "chapter": chapter['title'],
                        "position": f"ç¬¬{chapter['chapter_id']+1}ç«  - ç¬¬{chunk['chunk_id']+1}èŠ‚"
                    }
        return None

    def get_weak_points(self, subject: str, tracker, limit: int = 5) -> list:
        weak_ids = tracker.get_weak_knowledge(subject, limit)
        result = []
        for kid, score in weak_ids:
            k = self.get_knowledge_by_id(kid)
            if k:
                k['last_score'] = score
                result.append(k)
        return result

    def get_random_knowledge(self, subject: str = None) -> dict:
        try:
            collection = self.vectorstore._collection
            where_filter = {"subject": subject} if subject else None
            results = collection.get(where=where_filter, include=['metadatas', 'documents'])

            if not results['ids']:
                return None

            idx = random.randint(0, len(results['ids']) - 1)
            return {
                "content": results['documents'][idx],
                "metadata": results['metadatas'][idx],
                "id": results['ids'][idx]
            }
        except:
            return None
